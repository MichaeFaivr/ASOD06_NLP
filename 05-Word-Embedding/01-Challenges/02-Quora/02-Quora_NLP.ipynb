{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### target: either 1 (for insincere question) or 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (3.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sentence-transformers) (4.34.0)\n",
      "Requirement already satisfied: tqdm in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sentence-transformers) (1.23.4)\n",
      "Requirement already satisfied: scikit-learn in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: scipy in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sentence-transformers) (1.8.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sentence-transformers) (0.16.4)\n",
      "Requirement already satisfied: Pillow in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sentence-transformers) (9.1.1)\n",
      "Requirement already satisfied: filelock in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2022.10.0)\n",
      "Requirement already satisfied: requests in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: sympy in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.8.7)\n",
      "Requirement already satisfied: jinja2 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers) (12.2.140)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.9.13)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.3.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.15.1->sentence-transformers) (3.0.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2022.9.24)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/michaelfaivre/.pyenv/versions/3.10.12/envs/artefact/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from textblob import TextBlob\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Quora/quora_train.csv',on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15000, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sample the dataset\n",
    "df= resample(df, n_samples=15000, random_state=42)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['qid', 'question_text', 'target'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum() #86 rows\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    14002\n",
       "1      912\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target.value_counts()\n",
    "# imbalanced dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilisation de Word Embedding:\n",
    "### on va procéder comme dans l'Exo-1 avec du rebalancing sur 1 avec du word embedding\n",
    "### sur les tokens avec glove.6B.100.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    # 1- tokenization\n",
    "    tokens = word_tokenize(document)\n",
    "    # 2- lower case of strings\n",
    "    tokens = [t.lower() if t.isalpha() else t for t in tokens]\n",
    "    # 3- remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens = [t for t in tokens if not t in stop_words]\n",
    "    # 4- Stemming\n",
    "    stemmer = PorterStemmer() #build root by removing some known suffix and prefix\n",
    "    tokens = [stemmer.stem(w) for w in tokens]\n",
    "    # 4- lemmatization\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #tokens_lem = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>17df11bf9b1888d64add</td>\n",
       "      <td>I feel something missing in my relationship. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>[feel, someth, miss, relationship, ., dont, kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671155</th>\n",
       "      <td>837181f0407fd112a05c</td>\n",
       "      <td>What do Socrates, Thomas Kuhn and Karl Popper ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[socrat, ,, thoma, kuhn, karl, popper, common, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>19d09fe4bbabf16eb347</td>\n",
       "      <td>What song played in the movie of the gifted wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[song, play, movi, gift, mr, bern, carson, lit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>32b9bf37edd829ffe658</td>\n",
       "      <td>What are the biggest myths about Adolf Hitler?</td>\n",
       "      <td>0</td>\n",
       "      <td>[biggest, myth, adolf, hitler, ?]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>1595cbbad9c20d3d791a</td>\n",
       "      <td>How can the ViewSonic PA503S 3600 lumens SVGA ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[viewson, pa503, 3600, lumen, svga, hdmi, proj...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "121958  17df11bf9b1888d64add   \n",
       "671155  837181f0407fd112a05c   \n",
       "131932  19d09fe4bbabf16eb347   \n",
       "259178  32b9bf37edd829ffe658   \n",
       "110268  1595cbbad9c20d3d791a   \n",
       "\n",
       "                                            question_text  target  \\\n",
       "121958  I feel something missing in my relationship. I...       0   \n",
       "671155  What do Socrates, Thomas Kuhn and Karl Popper ...       0   \n",
       "131932  What song played in the movie of the gifted wh...       0   \n",
       "259178     What are the biggest myths about Adolf Hitler?       0   \n",
       "110268  How can the ViewSonic PA503S 3600 lumens SVGA ...       0   \n",
       "\n",
       "                                                   tokens  \n",
       "121958  [feel, someth, miss, relationship, ., dont, kn...  \n",
       "671155  [socrat, ,, thoma, kuhn, karl, popper, common, ?]  \n",
       "131932  [song, play, movi, gift, mr, bern, carson, lit...  \n",
       "259178                  [biggest, myth, adolf, hitler, ?]  \n",
       "110268  [viewson, pa503, 3600, lumen, svga, hdmi, proj...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokens'] = df.question_text.apply(preprocessing)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENTIMENT ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>17df11bf9b1888d64add</td>\n",
       "      <td>I feel something missing in my relationship. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>[feel, someth, miss, relationship, ., dont, kn...</td>\n",
       "      <td>-0.253125</td>\n",
       "      <td>0.440625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671155</th>\n",
       "      <td>837181f0407fd112a05c</td>\n",
       "      <td>What do Socrates, Thomas Kuhn and Karl Popper ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[socrat, ,, thoma, kuhn, karl, popper, common, ?]</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>19d09fe4bbabf16eb347</td>\n",
       "      <td>What song played in the movie of the gifted wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[song, play, movi, gift, mr, bern, carson, lit...</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>32b9bf37edd829ffe658</td>\n",
       "      <td>What are the biggest myths about Adolf Hitler?</td>\n",
       "      <td>0</td>\n",
       "      <td>[biggest, myth, adolf, hitler, ?]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>1595cbbad9c20d3d791a</td>\n",
       "      <td>How can the ViewSonic PA503S 3600 lumens SVGA ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[viewson, pa503, 3600, lumen, svga, hdmi, proj...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "121958  17df11bf9b1888d64add   \n",
       "671155  837181f0407fd112a05c   \n",
       "131932  19d09fe4bbabf16eb347   \n",
       "259178  32b9bf37edd829ffe658   \n",
       "110268  1595cbbad9c20d3d791a   \n",
       "\n",
       "                                            question_text  target  \\\n",
       "121958  I feel something missing in my relationship. I...       0   \n",
       "671155  What do Socrates, Thomas Kuhn and Karl Popper ...       0   \n",
       "131932  What song played in the movie of the gifted wh...       0   \n",
       "259178     What are the biggest myths about Adolf Hitler?       0   \n",
       "110268  How can the ViewSonic PA503S 3600 lumens SVGA ...       0   \n",
       "\n",
       "                                                   tokens  polarity  \\\n",
       "121958  [feel, someth, miss, relationship, ., dont, kn... -0.253125   \n",
       "671155  [socrat, ,, thoma, kuhn, karl, popper, common, ?] -0.300000   \n",
       "131932  [song, play, movi, gift, mr, bern, carson, lit...  0.156250   \n",
       "259178                  [biggest, myth, adolf, hitler, ?]  0.000000   \n",
       "110268  [viewson, pa503, 3600, lumen, svga, hdmi, proj...  0.000000   \n",
       "\n",
       "        subjectivity  \n",
       "121958      0.440625  \n",
       "671155      0.500000  \n",
       "131932      0.750000  \n",
       "259178      0.000000  \n",
       "110268      0.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"polarity\"]     = df[\"question_text\"].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df[\"subjectivity\"] = df[\"question_text\"].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>tokens</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>17df11bf9b1888d64add</td>\n",
       "      <td>I feel something missing in my relationship. I...</td>\n",
       "      <td>0</td>\n",
       "      <td>[feel, someth, miss, relationship, ., dont, kn...</td>\n",
       "      <td>-0.253125</td>\n",
       "      <td>0.440625</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>671155</th>\n",
       "      <td>837181f0407fd112a05c</td>\n",
       "      <td>What do Socrates, Thomas Kuhn and Karl Popper ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[socrat, ,, thoma, kuhn, karl, popper, common, ?]</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>19d09fe4bbabf16eb347</td>\n",
       "      <td>What song played in the movie of the gifted wh...</td>\n",
       "      <td>0</td>\n",
       "      <td>[song, play, movi, gift, mr, bern, carson, lit...</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>32b9bf37edd829ffe658</td>\n",
       "      <td>What are the biggest myths about Adolf Hitler?</td>\n",
       "      <td>0</td>\n",
       "      <td>[biggest, myth, adolf, hitler, ?]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>1595cbbad9c20d3d791a</td>\n",
       "      <td>How can the ViewSonic PA503S 3600 lumens SVGA ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[viewson, pa503, 3600, lumen, svga, hdmi, proj...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         qid  \\\n",
       "121958  17df11bf9b1888d64add   \n",
       "671155  837181f0407fd112a05c   \n",
       "131932  19d09fe4bbabf16eb347   \n",
       "259178  32b9bf37edd829ffe658   \n",
       "110268  1595cbbad9c20d3d791a   \n",
       "\n",
       "                                            question_text  target  \\\n",
       "121958  I feel something missing in my relationship. I...       0   \n",
       "671155  What do Socrates, Thomas Kuhn and Karl Popper ...       0   \n",
       "131932  What song played in the movie of the gifted wh...       0   \n",
       "259178     What are the biggest myths about Adolf Hitler?       0   \n",
       "110268  How can the ViewSonic PA503S 3600 lumens SVGA ...       0   \n",
       "\n",
       "                                                   tokens  polarity  \\\n",
       "121958  [feel, someth, miss, relationship, ., dont, kn... -0.253125   \n",
       "671155  [socrat, ,, thoma, kuhn, karl, popper, common, ?] -0.300000   \n",
       "131932  [song, play, movi, gift, mr, bern, carson, lit...  0.156250   \n",
       "259178                  [biggest, myth, adolf, hitler, ?]  0.000000   \n",
       "110268  [viewson, pa503, 3600, lumen, svga, hdmi, proj...  0.000000   \n",
       "\n",
       "        subjectivity  sentiment_textblob  \n",
       "121958      0.440625                   0  \n",
       "671155      0.500000                   0  \n",
       "131932      0.750000                   1  \n",
       "259178      0.000000                   0  \n",
       "110268      0.000000                   0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sentiment_textblob\"] = (df.polarity > 0).astype(int)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.64      0.77     14002\n",
      "           1       0.07      0.39      0.12       912\n",
      "\n",
      "    accuracy                           0.63     14914\n",
      "   macro avg       0.50      0.52      0.44     14914\n",
      "weighted avg       0.89      0.63      0.73     14914\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# unsupervised predictions with textblob\n",
    "# 12% on insincere\n",
    "print(classification_report(df[\"target\"], df[\"sentiment_textblob\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14914,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Build initial X and y\n",
    "X = df.tokens\n",
    "y = df.target.to_numpy()\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitter avant sur cette étape pour ne pas introduire de Data Leakage !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (11931,)\n",
      "X_test.shape: (2983,)\n",
      "y_train.shape: (11931,)\n",
      "y_test.shape: (2983,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "print(f'X_train.shape: {X_train.shape}')\n",
    "print(f'X_test.shape: {X_test.shape}')\n",
    "print(f'y_train.shape: {y_train.shape}')\n",
    "print(f'y_test.shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "434368               [approach, beauti, women, street, ,, ?]\n",
       "671088                        [long, induct, light, last, ?]\n",
       "219073     [data, scientist, learn, write, front-end, cod...\n",
       "647534                  [revers, irrevers, chang, differ, ?]\n",
       "331999         [n't, germani, occupi, end, world, war, 1, ?]\n",
       "66197      [respons, salari, cse, student, recruit, gate,...\n",
       "1059578    ['s, best, thing, 've, ever, experienc, around...\n",
       "104555                     [much, freight, forward, make, ?]\n",
       "456610     [1984, version, ``, red, dawn, ,, '', scenario...\n",
       "839665                                           [hernia, ?]\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(730,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "104555                     [much, freight, forward, make, ?]\n",
       "1126998    [malaysian, incompet, mani, job, like, restaur...\n",
       "870793                                  [jew, scare, dog, ?]\n",
       "464535     [liber, need, day, school, safe, space, color,...\n",
       "1211970                    [beat, kany, west, 's, stupid, ?]\n",
       "                                 ...                        \n",
       "301454     [mani, notic, speci, speci, get, extinct, surr...\n",
       "306024     [alway, clash, hindu, muslim, religion, like, ...\n",
       "823622     [donald, trump, like, die, impeach, novemb, 20...\n",
       "679706       [arab, ,, black, hispan, stereotyp, violent, ?]\n",
       "1167498    [dumb, thought, doctor, told, get, extra, fri,...\n",
       "Name: tokens, Length: 730, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : isolate the insincere reviews in a variable\n",
    "##tokenized_insincere_reviews = df[df['target']==1].tokens\n",
    "tokenized_insincere_reviews = X_train[y_train==1]\n",
    "print(tokenized_insincere_reviews.shape)\n",
    "tokenized_insincere_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a word embedding (Glove) to create your corpus and run your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_filename = 'Quora/glove/glove.6B.50d.txt'\n",
    "model = KeyedVectors.load_word2vec_format(glove_filename, binary=False, no_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50451 ,  0.68607 , -0.59517 , -0.022801,  0.60046 , -0.13498 ,\n",
       "       -0.08813 ,  0.47377 , -0.61798 , -0.31012 , -0.076666,  1.493   ,\n",
       "       -0.034189, -0.98173 ,  0.68229 ,  0.81722 , -0.51874 , -0.31503 ,\n",
       "       -0.55809 ,  0.66421 ,  0.1961  , -0.13495 , -0.11476 , -0.30344 ,\n",
       "        0.41177 , -2.223   , -1.0756  , -1.0783  , -0.34354 ,  0.33505 ,\n",
       "        1.9927  , -0.04234 , -0.64319 ,  0.71125 ,  0.49159 ,  0.16754 ,\n",
       "        0.34344 , -0.25663 , -0.8523  ,  0.1661  ,  0.40102 ,  1.1685  ,\n",
       "       -1.0137  , -0.21585 , -0.15155 ,  0.78321 , -0.91241 , -1.6106  ,\n",
       "       -0.64426 , -0.51042 ], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create new reviews for train corpus from insincere reviews on NN flagged tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104555     [(much, JJ), (freight, NN), (forward, RB), (ma...\n",
       "1126998    [(malaysian, JJ), (incompet, NN), (mani, JJ), ...\n",
       "870793           [(jew, NN), (scare, NN), (dog, NN), (?, .)]\n",
       "464535     [(liber, NNS), (need, VBP), (day, NN), (school...\n",
       "1211970    [(beat, NN), (kany, JJ), (west, NN), ('s, POS)...\n",
       "                                 ...                        \n",
       "301454     [(mani, NN), (notic, JJ), (speci, NN), (speci,...\n",
       "306024     [(alway, RB), (clash, JJ), (hindu, NN), (musli...\n",
       "823622     [(donald, JJ), (trump, NN), (like, IN), (die, ...\n",
       "679706     [(arab, NN), (,, ,), (black, JJ), (hispan, NN)...\n",
       "1167498    [(dumb, JJ), (thought, VBD), (doctor, NN), (to...\n",
       "Name: tokens, Length: 730, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO : add the POS-tag to all spam tokens\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "pos_tag_insincere = tokenized_insincere_reviews.apply(nltk.pos_tag)\n",
    "pos_tag_insincere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voir pourquoi les noms propres sont aussi transformés !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104555                        [much, rail, forward, make, ?]\n",
       "1126998    [malaysian, incompet, mani, getting, like, res...\n",
       "870793                                [jews, scares, cat, ?]\n",
       "464535     [liber, need, days, college, safe, earth, colo...\n",
       "1211970                 [beating, kany, east, 's, stupid, ?]\n",
       "                                 ...                        \n",
       "301454     [ratnam, notic, speci, speci, get, extinct, re...\n",
       "306024     [alway, clash, hindus, muslims, beliefs, like,...\n",
       "823622     [donald, casino, like, die, impeachment, novem...\n",
       "679706     [muslim, ,, black, hispan, stereotyp, violence...\n",
       "1167498    [dumb, thought, nurse, told, get, extra, tue, ...\n",
       "Name: tokens, Length: 730, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## same as in Exo-1\n",
    "# replace token with the top 1 most similar word if 2 conditions are met:\n",
    "# the POS-tag == 'NN' and the token has an embedding.\n",
    "## Pb avec l'error: AttributeError: The vocab attribute was removed from KeyedVector in Gensim 4.0.0.\n",
    "vocab = model.key_to_index\n",
    "vocab\n",
    "def replace_to_synonyme(tagged_tokens):\n",
    "    ## issue with model.vocab\n",
    "    tokens = [t_pos[0] if t_pos[1]!='NN' or t_pos[0] not in vocab\n",
    "              else model.most_similar(t_pos[0], topn = 1)[0][0] for t_pos in tagged_tokens]\n",
    "    return tokens\n",
    "\n",
    "synonymes_tokens = pos_tag_insincere.apply(replace_to_synonyme)\n",
    "synonymes_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['theist', 'explain', 'venu', 'boil', 'hellish', 'nightmar', 'mar', 'frozen', 'desert', '?', 'would', \"n't\", 'god', 'want', 'lot', 'worship', '?']\n",
      "['technophile', 'understand', 'nedumudi', 'simmer', 'hellish', 'nightmar', 'zheye', 'dried', 'jungle', '?', 'would', \"n't\", 'god', 'want', 'really', 'sacred', '?']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_insincere_reviews.iloc[150])\n",
    "print(synonymes_tokens.iloc[150])\n",
    "\n",
    "### 'virgin' -> remplacé par 'branson' !!?? (ex-patron de Virgin): PB à voir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11931,)\n",
      "(12661,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_627009/3322240715.py:5: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_train = X_train.append(pd.Series(synonymes_tokens), ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                  [approach, beauti, women, street, ,, ?]\n",
       "1                           [long, induct, light, last, ?]\n",
       "2        [data, scientist, learn, write, front-end, cod...\n",
       "3                     [revers, irrevers, chang, differ, ?]\n",
       "4            [n't, germani, occupi, end, world, war, 1, ?]\n",
       "                               ...                        \n",
       "12656    [ratnam, notic, speci, speci, get, extinct, re...\n",
       "12657    [alway, clash, hindus, muslims, beliefs, like,...\n",
       "12658    [donald, casino, like, die, impeachment, novem...\n",
       "12659    [muslim, ,, black, hispan, stereotyp, violence...\n",
       "12660    [dumb, thought, nurse, told, get, extra, tue, ...\n",
       "Name: tokens, Length: 12661, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# append to X new tokens from insincere labelled texts to Train set !!\n",
    "# voir Exo-1\n",
    "# add the newly generated insincere texts to the corpus\n",
    "print(X_train.shape)\n",
    "X_train = X_train.append(pd.Series(synonymes_tokens), ignore_index=True)\n",
    "print(X_train.shape)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12661,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_627009/1440348975.py:4: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  y_train = y_train.append(pd.Series(np.ones(len(synonymes_tokens))), ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        0.0\n",
       "1        0.0\n",
       "2        0.0\n",
       "3        0.0\n",
       "4        0.0\n",
       "        ... \n",
       "12656    1.0\n",
       "12657    1.0\n",
       "12658    1.0\n",
       "12659    1.0\n",
       "12660    1.0\n",
       "Length: 12661, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as in Exo-1 add new labels to your `y` variable\n",
    "## all class==1 as they are new spams\n",
    "y_train = pd.Series(y_train)\n",
    "y_train = y_train.append(pd.Series(np.ones(len(synonymes_tokens))), ignore_index=True)\n",
    "print(y_train.shape)\n",
    "y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11201\n",
      "1460\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[y_train==0]))\n",
    "print(len(y_train[y_train==1]))\n",
    "#0.9-to-7 -> rebalancing of target 1 by a factor of 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12661, 14047)\n",
      "(12661,)\n"
     ]
    }
   ],
   "source": [
    "# TFIDF trained on train subset\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tf_idf_train = vectorizer.fit_transform(X_train).toarray()\n",
    "tf_idf_test = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "print(tf_idf_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "## dim features > nb rows: pas cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=100)\n",
    "lr.fit(tf_idf_train, y_train)\n",
    "\n",
    "# prédictions sur Train et Test\n",
    "y_pred_train = lr.predict(tf_idf_train)\n",
    "y_pred_test  = lr.predict(tf_idf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " F1-score sur Train - sincere: 0.9584818170108625\n",
      " F1-score sur Train - insincere: 0.5238798621368784\n",
      " F1-score sur Test - sincere: 0.9703522846180677\n",
      " F1-score sur Test - insincere: 0.26724137931034486\n"
     ]
    }
   ],
   "source": [
    "# check the F1-score on the minority class\n",
    "from sklearn.metrics import f1_score,recall_score,precision_score\n",
    "\n",
    "# F1-score de Train\n",
    "vect_f1_score_train = f1_score(y_train, y_pred_train, average=None)\n",
    "print(f\" F1-score sur Train - sincere: {vect_f1_score_train[0]}\")\n",
    "print(f\" F1-score sur Train - insincere: {vect_f1_score_train[1]}\")\n",
    "# F1-score de Test\n",
    "vect_f1_score_test = f1_score(y_test, y_pred_test, average=None)\n",
    "print(f\" F1-score sur Test - sincere: {vect_f1_score_test[0]}\")\n",
    "print(f\" F1-score sur Test - insincere: {vect_f1_score_test[1]}\")\n",
    "# 10000 au départ:\n",
    "# 0.079 sans ajouts des new insincere texts\n",
    "# 0.21 avec ajouts des new insincere texts\n",
    "# 15000 au départ:\n",
    "# 0.235 avec ajouts des new insincere texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avec XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'XGBClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xgb_classifier \u001b[38;5;241m=\u001b[39m \u001b[43mXGBClassifier\u001b[49m(eta \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m      2\u001b[0m xgb_classifier\u001b[38;5;241m.\u001b[39mfit(tf_idf_train, y_train)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# prédictions sur Train et Test\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "xgb_classifier = XGBClassifier(eta = 0.7)\n",
    "xgb_classifier.fit(tf_idf_train, y_train)\n",
    "\n",
    "# prédictions sur Train et Test\n",
    "y_pred_train = xgb_classifier.predict(tf_idf_train)\n",
    "y_pred_test  = xgb_classifier.predict(tf_idf_test)\n",
    "\n",
    "# F-scores\n",
    "vect_f1_score_train = f1_score(y_train, y_pred_train, average=None)\n",
    "print(f\" F1-score sur Train - sincere: {vect_f1_score_train[0]}\")\n",
    "print(f\" F1-score sur Train - insincere: {vect_f1_score_train[1]}\")\n",
    "# F1-score de Test\n",
    "vect_f1_score_test = f1_score(y_test, y_pred_test, average=None)\n",
    "print(f\" F1-score sur Test - sincere: {vect_f1_score_test[0]}\")\n",
    "print(f\" F1-score sur Test - insincere: {vect_f1_score_test[1]}\")\n",
    "\"\"\"\n",
    "# F1-score sur Train - sincere: 0.9812573725370266\n",
    "# F1-score sur Train - insincere: 0.8337853545137544\n",
    "# F1-score sur Test - sincere: 0.9672790901137358\n",
    "# F1-score sur Test - insincere: 0.34385964912280703\n",
    "\"\"\"\n",
    "\n",
    "# F-scores weighted on both classes\n",
    "f1_score_test = f1_score(y_test, y_pred_test, average='weighted')\n",
    "print(f\" F1-score sur Test: {f1_score_test}\")\n",
    "# F1-score sur Test: 0.9290426977329589 - mais faible sur target 1 ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### avec sentence_transformer à la place de word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121958     I feel something missing in my relationship. I...\n",
       "671155     What do Socrates, Thomas Kuhn and Karl Popper ...\n",
       "131932     What song played in the movie of the gifted wh...\n",
       "259178        What are the biggest myths about Adolf Hitler?\n",
       "110268     How can the ViewSonic PA503S 3600 lumens SVGA ...\n",
       "                                 ...                        \n",
       "913107     How will I do after finish BA in political sci...\n",
       "454318                   How intelligent is Marc Andreessen?\n",
       "1187939    Did Kylo Ren castrate himself when he turned t...\n",
       "278429     If secularists don't want religion to be invol...\n",
       "1009926    Which topic on Quora has the most number of fo...\n",
       "Name: question_text, Length: 14914, dtype: object"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.question_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09074124,  0.03276177, -0.00510913, ...,  0.0103267 ,\n",
       "        -0.01616477,  0.1584273 ],\n",
       "       [-0.04947783,  0.10102484, -0.2364795 , ..., -0.03998633,\n",
       "         0.06539483,  0.324195  ],\n",
       "       [-0.1569635 ,  0.01024112, -0.02797146, ..., -0.04710345,\n",
       "        -0.17310087,  0.03577788],\n",
       "       ...,\n",
       "       [ 0.05709595,  0.01944001,  0.1399895 , ...,  0.3851375 ,\n",
       "        -0.2661875 ,  0.30452874],\n",
       "       [ 0.17428964, -0.13716224,  0.1388662 , ...,  0.16437475,\n",
       "        -0.08975447, -0.05332353],\n",
       "       [-0.35225925,  0.011895  ,  0.234319  , ...,  0.093244  ,\n",
       "        -0.0595547 , -0.36777502]], dtype=float32)"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = df.question_text.to_list()\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/average_word_embeddings_glove.6B.300d')\n",
    "## l'arg doit être une liste\n",
    "embeddings = model.encode(sentences)\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### que faire avec embeddings from sentence_transformer ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14914, 300)"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### voir s'il faut aussi appliquer sentence_transformer après le split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14914, 300)"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = embeddings\n",
    "y = df.target\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (11931, 300)\n",
      "X_test.shape: (2983, 300)\n",
      "y_train.shape: (11931,)\n",
      "y_test.shape: (2983,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,stratify=y)\n",
    "print(f'X_train.shape: {X_train.shape}')\n",
    "print(f'X_test.shape: {X_test.shape}')\n",
    "print(f'y_train.shape: {y_train.shape}')\n",
    "print(f'y_test.shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter=100)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# prédictions sur Train et Test\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test  = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " F1-score sur Train - sincere: 0.9702053298383574\n",
      " F1-score sur Train - insincere: 0.29835390946502055\n",
      " F1-score sur Test - sincere: 0.9674597620713786\n",
      " F1-score sur Test - insincere: 0.256\n"
     ]
    }
   ],
   "source": [
    "# F-scores\n",
    "vect_f1_score_train = f1_score(y_train, y_pred_train, average=None)\n",
    "print(f\" F1-score sur Train - sincere: {vect_f1_score_train[0]}\")\n",
    "print(f\" F1-score sur Train - insincere: {vect_f1_score_train[1]}\")\n",
    "# F1-score de Test\n",
    "vect_f1_score_test = f1_score(y_test, y_pred_test, average=None)\n",
    "print(f\" F1-score sur Test - sincere: {vect_f1_score_test[0]}\")\n",
    "print(f\" F1-score sur Test - insincere: {vect_f1_score_test[1]}\")\n",
    "# c'est moins bien qu'avec word-embedding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artefact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
