{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: preprocess and compute the TF-IDF of this database\n",
    "def preprocessing(document):\n",
    "    # 1- tokenization\n",
    "    tokens = word_tokenize(document)\n",
    "    # 2- lower case on alpha and leave unchaged others\n",
    "    ###tokens = [t.lower() if t.isalpha() else t for t in tokens] ## pb sur similarity.max(): à voir\n",
    "    # 3- remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "    tokens = [t for t in tokens if not t in stop_words]\n",
    "    # 4- stemming\n",
    "    stemmer = PorterStemmer() #build root by removing some known suffix and prefix\n",
    "    tokens_stem = [stemmer.stem(w) for w in tokens]\n",
    "    return tokens_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## base d'entraînement de vectorizer: Asource + Bsource + Csource + Dsource -> database\n",
    "def read_data_preproc_dataframe():\n",
    "    files_ids = ['A','B','C','D']\n",
    "\n",
    "    # database is a txt file\n",
    "    # in the same rep as the code to make it easier\n",
    "    liste_sentences = []\n",
    "    for id_letter in files_ids:\n",
    "        database = id_letter + 'source.txt'\n",
    "        with open(database, 'r') as file:\n",
    "            data = file.read()\n",
    "            liste_sentences.append(data)\n",
    "\n",
    "    # Build the dataframe from the list of sentences\n",
    "    df = pd.DataFrame(data=liste_sentences)\n",
    "    df.columns\n",
    "    df.columns = ['Text']\n",
    "    #df['Text_preproc'] = df['Text'].apply(preprocessing) ## pas utilisé en fait\n",
    "    print(df)\n",
    "    print(f'df.shape: {df.shape}')\n",
    "\n",
    "    ## TF-IDF entraîné sur tout le corpus\n",
    "    ## !!! OK avec le preproc de base de sklearn sur col: 'Text' pour la fonction: get_closest_sentence() !!!\n",
    "    TFIDF_vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'))\n",
    "    TF_IDF = TFIDF_vectorizer.fit_transform(df.Text).toarray()\n",
    "\n",
    "    return TFIDF_vectorizer, TF_IDF, df.Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text\n",
      "0  In ages which have no record these islands wer...\n",
      "1  No oil spill is entirely benign. Depending on ...\n",
      "2  Descartes has been heralded as the first moder...\n",
      "3  Normal science, the activity in which most sci...\n",
      "df.shape: (4, 1)\n"
     ]
    }
   ],
   "source": [
    "TFIDF_vectorizer, TF_IDF, df_Text = read_data_preproc_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## copié de EXO-5\n",
    "def get_closest_sentence(query, tf_idf, vectorizer, df_text):\n",
    "    closest_sentence = \"\"\n",
    "    query_TFIDF = vectorizer.transform(query).toarray()\n",
    "    #print(pd.DataFrame(data=query_TFIDF, columns=TFIDF_vectorizer.get_feature_names_out()))\n",
    "\n",
    "    # cosine similarity between the query and the movies\n",
    "    similarity = cosine_similarity(query_TFIDF, tf_idf)\n",
    "\n",
    "    #print(f\"Similarity.max(): {similarity.max()}\") #MAX = 0.0 dans les tests\n",
    "    #print(f'similarity.shape: {similarity.shape}')\n",
    "    #print(f'similarity.argmax(): {similarity.argmax()}')\n",
    "\n",
    "    closest_sentence = df_text[similarity.argmax()]\n",
    "    print(f\"phrase la + proche de ** {query[0]} **:\\n {closest_sentence}\")\n",
    "    return similarity.max(), closest_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "artefact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
